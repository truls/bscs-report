\chapter{Analysis and Design}

\section{Motivation}

\section{Synchronous Message Exchange}

\subsection{Components}

In this section, we describe the various components that make up the
a SME network.

\begin{description}
  \item[Process] 
\end{description}

\subsection{Properties}
The SME model has a number of special properties which influences how
to efficiently represent and execute the network in our implementation.

\begin{description}

  \item[Clock cycles] One defining feature of
    hardware is that all processing is driven by a clock beat. In
    order to enable fulfillment of this goal we introduce a simulated
    clock beat in our implementation of SME and thus the defining
    property of hardware is preserved in the SME model.

  \item[Global synchronicity] As a consequence of implementing the
    simulated clock best, all events and communications of the
    network occurs completely synchronous from the point of view of a
    process. FIXME

  \item[Shared nothing] A process can be considered a completely
    autonomous and can only change state through receiving a message
    on its incoming bus. A process is also self contained in the sense
\end{description}

\subsection{Execution model}
Our execution is derived from the unique properties of the system

\section{Public API}
The API exposed to the user of the SME library should be designed with
a focus on balancing expressiveness and ability to be easily
understanidble....

The API implemented by \cite{vinter2014synchronous} is heavily
dependent of the highly dynamic nature of the Python programming
language used to implement their prototype. Since our implementation
is written in C++ we cannot directly mimic the python API in our
implementation. Given the similarities between SME and CSP it seems
obvious to let us inspire by 

\section{Problem characteristics}

\section{Paralellization model}

Using OS-level threads for representing the SME processes was quickly
ruled out during the design process. OS-level threads are severely
limited in that the number of threads a process can have is highly
limited \fxnote{Find reference for number of threads per process} and
switching between OS-level threads is very expensive compared to, for
instance, user-level threads. \cite{sung2002comparative}\fxnote{Find
  more recent citation for the performance of OS vs. user-level threads}.

Using user-level threads, on linux implemented by using the (now
deprecated)  \texttt{setjump, longjump} functions or the more current \texttt{
setcontext, getcontext}, is another possible way of implementing our
networks. The notion of a user-level thread is highly compatible with
our concept of a process, namely \fxnote{finish}. User-level threads is the
method that is used to implement many CSP libraries, including
C++CSP2.

One important difference between the CSP and SME execution models is
that CSP is highly asynchronous in nature while SME is, as previously
mentioned, entirely synchronous. This allows us to take a
significantly more simple approach to scheduling threads compared to
C++CSP. A parallel CSP library needs to schedule a process for
execution based on events generated by other processes
\cite{brown2003introduction}. In SME we know that every process needs
to run \fxnote{This needs to be better described} during every ``clock
cycle''. This greatly simplifies our implementation of multi-threaded
SME.

Based on this, we propose an extremely and efficient model for
parallel execution of a SME process network illustrated
in. \fxnote{make figure similar to \cref{fig:subopt-dist}}. The idea
is to start one OS-level thread per available CPU-core and assign a
subset of SME processes to each code. Then, for each iteration of the
SME-network \fxnote{Define ``iteration of SME-network'' as a full
clock-cycle of the simulated hardware or replace with different
term}, the controlling thread will start by 



\section{Identifying optimal process scheduling}
In order to determine the efficiency of various methods of process
scheduling we need to identify the optimality condition for our
process scheduling. An illustration of our threading model can be seen in
\cref{fig:subopt-dist}. The green boxes represents processes while the
red boxes indicates core idle time. Notice, how we by redistributing
the processes across threads could reduce the idle-time of our
cores.

\begin{figure}
\centering
\includegraphics{figures/parallel}
\caption[Proposed SME parallelization model]{Example of suboptimal
  distribution of processes across processing threads. Green blocks
  represents processes while red blocks represents thread idle
  time. Threads are named $p_{i,j}$ where $i$ is the number of the
  thread the process has been assigned to and $f_i$ is the combined
  idle time for each thread. Threads are named $t_i$.}

\label{fig:subopt-dist}

\end{figure}

\section{Process orchestration}
As we discussed in the previous section, the primary limiting factor
for our multi-threaded network is an uneven and suboptimal
distribution of processes across CPU-cores. If orchestration is
determined by the order of which processes were defined

The optimal way to do this depend

\section{One-shot process orchestration}
In this model, we orchestrate the processes in our network as soon as
possible after execution start and

\section{Monte Carlo orchestration}
In this approach, we simply randomize the order of the processes. The
main advantage of this approach is that is computationally cheap
compared to 

\section{Optimization-based orchestration}
Another way to orchestrate the processes is to use a 


\section{Adaptive process orchestration}
The benefits of using a oneshot orchestration approach diminishes when
we execute process networks where the processes performs a variable
amount of work per iteration. In these kinds of networks, CPU-core
load distribution will gradually become uneven and suboptimal as the
network execution progresses. In order to keep this from happening and
maximize CPU-core utilization, we need to monitor process execution
time and core idle time as the network execution progresses. This is
what we refer to as adaptive orchestration. This approach, however
introduces another trade-off that we need to consider. producing an 

\section{Adaptive Monte Carlo process orchestration}

\section{Adaptive Optimization-based process orchestration}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% TeX-command-extra-options: "-enable-write18"
%%% End:
